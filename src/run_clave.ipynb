{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with CLAVE\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/davidaf3/CLAVE/blob/master/src/run_clave.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook shows how you can run inference on CLAVE and creates a Gradio UI that lets you experiment with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the necessary dependencies. This only install the packages that are not available in Colab. If you are not using Colab, you might need to install `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rarfile gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone CLAVE's repo and move into it. If you are running this notebook locally and have already clone the repo, this step is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/davidaf3/CLAVE.git\n",
    "%cd CLAVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model weights\n",
    "First, download the model weights from the provided URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "\n",
    "res = requests.get(\n",
    "    \"https://www.reflection.uniovi.es/bigcode/download/2024/CLAVE/model.rar\",\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "with tqdm(\n",
    "    total=int(res.headers.get(\"content-length\", 0)), unit=\"B\", unit_scale=True\n",
    ") as progress_bar:\n",
    "    with open(\"model.rar\", \"wb\") as f:\n",
    "        for data in res.iter_content(1024):\n",
    "            progress_bar.update(len(data))\n",
    "            f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the downloaded `model.rar` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rarfile\n",
    "\n",
    "\n",
    "with rarfile.RarFile(\"model.rar\") as f:\n",
    "    f.extractall(path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the weights\n",
    "Create a new model (`FineTunedModel` class) and load the weights from the extracted file (`CLAVE.pt`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import FineTunedModel\n",
    "from tokenizer import SpTokenizer\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = FineTunedModel(\n",
    "    SpTokenizer.get_vocab_size(), 512, 512, 8, 2048, 6, use_layer_norm=True\n",
    ").to(device)\n",
    "model_checkpoint = torch.load(\"CLAVE.pt\", map_location=device)\n",
    "weights = {\n",
    "    k[10:] if k.startswith(\"_orig_mod\") else k: v\n",
    "    for k, v in model_checkpoint[\"model_state_dict\"].items()\n",
    "}\n",
    "model.load_state_dict(weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the UI\n",
    "Start the Gradio UI configured to run the `verify_authorship` function. This function tokenizes the inputs, processes the tokens with CLAVE to obtain an embedding for each input, and computes the distance between the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch.nn.functional as F\n",
    "from utils import pad_and_split_tokens\n",
    "\n",
    "\n",
    "tokenizer = SpTokenizer()\n",
    "threshold = 0.1050\n",
    "\n",
    "\n",
    "def verify_authorship(source_code_1, source_code_2):\n",
    "    with torch.inference_mode():\n",
    "        tokens_1 = pad_and_split_tokens(tokenizer.tokenizes(source_code_1))[0]\n",
    "        tokens_2 = pad_and_split_tokens(tokenizer.tokenizes(source_code_2))[0]\n",
    "        embedding_1 = model(torch.tensor([tokens_1], device=device))\n",
    "        embedding_2 = model(torch.tensor([tokens_2], device=device))\n",
    "        distance = (1 - F.cosine_similarity(embedding_1, embedding_2)).item()\n",
    "        return [\n",
    "            distance,\n",
    "            \"Yes\" if distance <= threshold else \"No\",\n",
    "        ]\n",
    "\n",
    "\n",
    "ui = gr.Interface(\n",
    "    fn=verify_authorship,\n",
    "    inputs=[\n",
    "        gr.Code(language=\"python\", label=\"Source code 1\"),\n",
    "        gr.Code(language=\"python\", label=\"Source code 2\"),\n",
    "    ],\n",
    "    outputs=[gr.Number(label=\"Distance\"), gr.Text(label=\"Same author?\")],\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "ui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
